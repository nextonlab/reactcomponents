## TiKV config template
##  Human-readable big numbers:
##   File size(based on byte): KB, MB, GB, TB, PB
##    e.g.: 1_048_576 = "1MB"
##   Time(based on ms): ms, s, m, h
##    e.g.: 78_000 = "1.3m"

## Log levels: trace, debug, info, warning, error, critical.
## Note that `debug` and `trace` are only available in development builds.
# log-level = "info"

## File to store logs.
## If it is not set, logs will be appended to stderr.
# log-file = ""

## File to store slow logs.
## If "log-file" is set, but this is not set, the slow logs will be appeneded
## to "log-file". If both "log-file" and "slow-log-file" are not set, all logs
## will be appended to stderr.
# slow-log-file = ""

## The minimum operation cost to output relative logs.
# slow-log-threshold = "1s"

## Timespan between rotating the log files.
## Once this timespan passes, log files will be rotated, i.e. existing log file will have a
## timestamp appended to its name and a new file will be created.
# log-rotation-timespan = "24h"

## Size of log file that triggers the log rotation.
## Once the size of log file exceeds the threshold value, the log file will be rotated
## and place the old log file in a new file named by orginal file name subbfixed by a timestamp.
# log-rotation-size = "300MB"

## The interval at which to refresh the local config
## Set it small are good to debug and tuning.
# refresh-config-interval = "30s"

[readpool]
## (Experimental) Whether to use a single thread pool to serve all the read requests. If it is
## set to `true`, the `readpool.storage` and `readpool.coprocessor` sections will be invalid and
## be ignored automatically.
# unify-read-pool = true

# Configurations for the single thread pool serving read requests.
# It only takes effect when `unify-read-pool` is `true`.
[readpool.unified]
## The minimal working thread count of the thread pool.
# min-thread-count = 1

## The maximum working thread count of the thread pool.
## The default value is max(4, LOGICAL_CPU_NUM * 0.8).
# max-thread-count = 8

## Size of the stack for each thread in the thread pool.
# stack-size = "10MB"

## Max running tasks of each worker, reject if exceeded.
# max-tasks-per-worker = 2000

[readpool.storage]
## Size of the thread pool for high-priority operations.
# high-concurrency = 4

## Size of the thread pool for normal-priority operations.
# normal-concurrency = 4

## Size of the thread pool for low-priority operations.
# low-concurrency = 4

## Max running high-priority operations of each worker, reject if exceeded.
# max-tasks-per-worker-high = 2000

## Max running normal-priority operations of each worker, reject if exceeded.
# max-tasks-per-worker-normal = 2000

## Max running low-priority operations of each worker, reject if exceeded.
# max-tasks-per-worker-low = 2000

## Size of the stack for each thread in the thread pool.
# stack-size = "10MB"

[readpool.coprocessor]
## Most read requests from TiDB are sent to the coprocessor of TiKV. high/normal/low-concurrency is
## used to set the number of threads of the coprocessor.
## If there are many read requests, you can increase these config values (but keep it within the
## number of system CPU cores). For example, for a 32-core machine deployed with TiKV, you can even
## set these config to 30 in heavy read scenarios.
## If CPU_NUM > 8, the default thread pool size for coprocessors is set to CPU_NUM * 0.8.

# high-concurrency = 8
# normal-concurrency = 8
# low-concurrency = 8
# max-tasks-per-worker-high = 2000
# max-tasks-per-worker-normal = 2000
# max-tasks-per-worker-low = 2000
# stack-size = "10MB"

[server]
## Listening address.
# addr = "127.0.0.1:20160"

## Advertise listening address for client communication.
## If not set, `addr` will be used.
# advertise-addr = ""

## Status address.
## This is used for reporting the status of TiKV directly through 
## the HTTP address. Notice that there is a risk of leaking status
## information if this port is exposed to the public.
## Empty string means disabling it.
# status-addr = "127.0.0.1:20180"

## Set the maximum number of worker threads for the status report HTTP service.
# status-thread-pool-size = 1

## Compression type for gRPC channel: none, deflate or gzip.
# grpc-compression-type = "none"

## Size of the thread pool for the gRPC server.
# grpc-concurrency = 4

## The number of max concurrent streams/requests on a client connection.
# grpc-concurrent-stream = 1024

## Limit the memory size can be used by gRPC. Default is unlimited.
## gRPC usually works well to reclaim memory by itself. Limit the memory in case OOM
## is observed. Note that limit the usage can lead to potential stall.
# grpc-memory-pool-quota = "32G"

## The number of connections with each TiKV server to send Raft messages.
# grpc-raft-conn-num = 1

## Amount to read ahead on individual gRPC streams.
# grpc-stream-initial-window-size = "2MB"

## Time to wait before sending out a ping to check if server is still alive.
## This is only for communications between TiKV instances.
# grpc-keepalive-time = "10s"

## Time to wait before closing the connection without receiving KeepAlive ping Ack.
# grpc-keepalive-timeout = "3s"

## How many snapshots can be sent concurrently.
# concurrent-send-snap-limit = 32

## How many snapshots can be received concurrently.
# concurrent-recv-snap-limit = 32

## Max allowed recursion level when decoding Coprocessor DAG expression.
# end-point-recursion-limit = 1000

## Max time to handle Coprocessor requests before timeout.
# end-point-request-max-handle-duration = "60s"

## Max bytes that snapshot can be written to disk in one second.
## It should be set based on your disk performance.
# snap-max-write-bytes-per-sec = "100MB"

## Whether to enable request batch.
# enable-request-batch = true

## Whether to collect batch across commands.
## When disabled, wait duration is ignored. When enabled, collect batch for specified duration
## when load is high.
# request-batch-enable-cross-command = true

## Wait duration before each request batch is processed. Wait is triggered when cross-command
## option is enabled and system load is high.
# request-batch-wait-duration = "1ms"

## Attributes about this server, e.g. `{ zone = "us-west-1", disk = "ssd" }`.
# labels = {}

[storage]
# Enable rawkv TTL
# refer to https://docs.google.com/document/d/1ygf8TdEMb_w4l6pyo3pHCdNV-JZ21koPfu1PHrP7Wc4/edit#heading=h.lw441vb2011
enable-ttl = true
ttl-check-poll-interval = "24h"

## The path to RocksDB directory.
# data-dir = "/tmp/tikv/store"

## The number of slots in Scheduler latches, which controls write concurrency.
## In most cases you can use the default value. When importing data, you can set it to a larger
## value.
# scheduler-concurrency = 2048000

## Scheduler's worker pool size, i.e. the number of write threads.
## It should be less than total CPU cores. When there are frequent write operations, set it to a
## higher value. More specifically, you can run `top -H -p tikv-pid` to check whether the threads
## named `sched-worker-pool` are busy.
# scheduler-worker-pool-size = 4

## When the pending write bytes exceeds this threshold, the "scheduler too busy" error is displayed.
# scheduler-pending-write-threshold = "100MB"

[storage.block-cache]
## Whether to create a shared block cache for all RocksDB column families.
##
## Block cache is used by RocksDB to cache uncompressed blocks. Big block cache can speed up read.
## It is recommended to turn on shared block cache. Since only the total cache size need to be
## set, it is easier to config. In most cases it should be able to auto-balance cache usage
## between column families with standard LRU algorithm.
##
## The rest of config in the storage.block-cache session is effective only when shared block cache
## is on.
# shared = true

## Size of the shared block cache. Normally it should be tuned to 30%-50% of system's total memory.
## When the config is not set, it is decided by the sum of the following fields or their default
## value:
##   * rocksdb.defaultcf.block-cache-size or 25% of system's total memory
##   * rocksdb.writecf.block-cache-size   or 15% of system's total memory
##   * rocksdb.lockcf.block-cache-size    or  2% of system's total memory
##   * raftdb.defaultcf.block-cache-size  or  2% of system's total memory
##
## To deploy multiple TiKV nodes on a single physical machine, configure this parameter explicitly.
## Otherwise, the OOM problem might occur in TiKV.
# capacity = "1GB"

[pd]
## PD endpoints.
# endpoints = []

## The interval at which to retry a PD connection initialization.
## Default is 300ms.
# retry-interval = "300ms"

## If the client observes an error, it can can skip reporting it except every `n` times.
## Set to 1 to disable this feature.
## Default is 10.
# retry-log-every = 10

## The maximum number of times to retry a PD connection initialization.
## Set to 0 to disable retry.
## Default is -1, meaning isize::MAX times.
# retry-max-count = -1

[raftstore]
## Whether to force to flush logs.
## Set to `true` (default) for best reliability, which prevents data loss when there is a power
## failure. Set to `false` for higher performance (ensure that you run multiple TiKV nodes!).
# sync-log = true

## Whether to enable Raft prevote.
## Prevote minimizes disruption when a partitioned node rejoins the cluster by using a two phase
## election.
# prevote = true

## The path to RaftDB directory.
## If not set, it will be `{data-dir}/raft`.
## If there are multiple disks on the machine, storing the data of Raft RocksDB on differen disks
## can improve TiKV performance.
# raftdb-path = ""

## Store capacity, i.e. max data size allowed.
## If it is not set, disk capacity is used.
# capacity = 0

## Internal notify capacity.
## 40960 is suitable for about 7000 Regions. It is recommended to use the default value.
# notify-capacity = 40960

## Maximum number of internal messages to process in a tick.
# messages-per-tick = 4096

## Region heartbeat tick interval for reporting to PD.
# pd-heartbeat-tick-interval = "60s"

## Store heartbeat tick interval for reporting to PD.
# pd-store-heartbeat-tick-interval = "10s"

## The threshold of triggering Region split check.
## When Region size change exceeds this config, TiKV will check whether the Region should be split
## or not. To reduce the cost of scanning data in the checking process, you can set the value to
## 32MB during checking and set it back to the default value in normal operations.
# region-split-check-diff = "6MB"

## The interval of triggering Region split check.
# split-region-check-tick-interval = "10s"

## When the number of Raft entries exceeds the max size, TiKV rejects to propose the entry.
# raft-entry-max-size = "8MB"

## Interval to GC unnecessary Raft log.
# raft-log-gc-tick-interval = "10s"

## Threshold to GC stale Raft log, must be >= 1.
# raft-log-gc-threshold = 50

